---
title: "Git and collaborative coding"
author: "Arthur de Grandpré"
date: "12 août 2020"
output: 
  html_document: 
    toc: yes
    toc_float: yes
---

```{r setup, include=F, echo=F, warning=FALSE, message=FALSE}
rm(list=ls())
gc()
library(png)
library(grid)
library(knitr)
library(ggplot2)
```



**Disclaimer: This document is material for a live 1h30 workshop and might feel incomplete out of it's context.**


# Why a workshop about collaborative coding and Git?

Previous workshops have been :   

- Dedicated to enhancing coding and data science level

- Largely based on the R environment  

- Giving strong numeracy bases for everyone attending  
  
But, as being mostly looking at code as ecologists, it feels like we still have yet to tackle some of basics of *coding hygiene* that should be expected in data science like:  

- How to ensure reproducibility of analysis   

- How to ensure safekeeping of data and scripts  

- How to collaborate efficiently in your code oriented projects  

May it be with yourself or with others, we need to be able to work efficiently to ensure clarity and reproducibility in science. This workshop is about tips and tricks to make your work more collab-friendly.


# Planning

The goal of this workshop is that by the end of the hour, attendees are able to figure out their own recipe for efficient collaborative coding.  
  
To do so, we will go through:  

- Basic annotation tips  

- How to structure scripts and projects efficiently  

- How to ensure safekeeping of scripts (version control, backups, etc.)  

- How to share your scripts and their results  
  
# Important note

```{r warningsign, echo=FALSE, fig.height=1.5, fig.width=2.5, fig.align='center'}
img1 = readPNG("images/warning.png")
grid.raster(img1)
```
   
There is no single recipe of preparing, cleaning and safekeeping your work. The following is just based on personnal experience and research, but the most important is to find your own way of being efficient with sufficient clarity for being able to share your work. 
see https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970 for more insight on the topic.    
  
This is mostly an effort to make sure everyone has some basic reflexes when working on their scripts.  

# Annotate for reproducibility

One of the pillars of science is reproducibility of results. Sadly, it is also one of the most neglected. Some authors even consider we are going through a reproducibility crisis, where a lot of the published science is actually quite obscure, with incomprehensible or unavailable methods and codes.

This section is mostly about how to make your code in a way that is understandable by others, but also yourself when you go back to it after a few months (or years) working on something else.

We have all written or recieved pieces of code that can look like the following:

```{r bad annotation, eval=FALSE}

r1=brick("data/test_CIR_clip_01.tif", crs="+init:epsg=26918") ; r1=r1/255
r1$ndvi=(r1$test_CIR_clip_01.1-r1$test_CIR_clip_01.2)/(r1$test_CIR_clip_01.1+r1$test_CIR_clip_01.2)
r2[r2$ndvi<(-0.45) | r2$test_CIR_clip_01.3>0.42]=-10
r3=r2$ndvi
r3[r3>(-10)]=1 ; r3[r3==(-10)]=0
p=rasterToPolygons(r3, dissolve=T)
p=disaggregate(p)
pr=rasterize(p,r3)
ps=PatchStat(pr)
p@data$patchID=rownames(p@data)
p2=merge(p,ps, by="patchID") ; p2=p2[complete.cases(p2@data),]
rc=rasterize(p2, r3, field="ndvi")
pv=p2[p2@data$ndvi==1,]
pr2=rasterize(pv, r3)
PS = PatchStat(pr2)
CS = ClassStat(rc)
```

Scripts that work, or have worked in the past, but that would be really difficult to use again since it contains absolutely no indication on what it's supposed to do, what is the data and how it is transformed.

There are many ways to tackle reproducibility of an analysis, the easiest is to think your code for it since $T_0$.  
  
This can be done by: 

- Working by **projects**
- Not procrastinating on the use of # *everywhere* to annotate your code (don't hesitate to overdescribe, even discuss results in some cases)  
- Planning your code like you would plan a written document (flowing, logical process)  
- Using Notebooks: R Markdown (Thanks *Lucas* for his workshop on the subject), Jupyter, LiveScript (for Matlab) 
- Writing your code in the most generic way possible (the less specific you are, the easier it will be to transfer your code)
- Writing your own functions, or even packages  


**Bottom line** : Code for future you if not for others


Looking back to the the previous exemple, many things can be modified to make it easier to handle. This is the same code, slightly modified to make collaborative coding easier.  

```{r exemple 1, eval=FALSE}
#### landscape metrics extraction ####

# input raster data
path_to_files = "./data/"
images = dir(path_to_files,
             pattern=".tif$",
             full.names=T) # this gives the full path of every tif files in the project's data folder

# read the first image, set the CRS to NAD83 UTM18N and bring the scale down to 0-1
r = brick(images[1], crs="+init:epsg=26918") ; r=r/255
# set the layer names to their actual color (THIS MIGHT CHANGED BASED ON YOUR DATA)
names(r) = ("nir","red","green")

# to calculate landscape metrics on vegetation patches, we need a mask of the vegetation
# that will be easier with a vegetation index
# calculate normalized difference vegetation index (NDVI) as a new layer of r
r$ndvi=(r$nir-r$red)/(r$nir+r$red)

# select vegetated pixels based on treshold values of green and ndvi, and make a binary image
# those values were obtained by manual treshold visualisation
ndvi_treshold = -0.45
green_treshold = 0.42

r2 = r
r2[r2$ndvi<ndvi_treshold | r2$green>green_treshold]=-10
r3=r2$ndvi
r3[r3>(-10)]=1 ; r3[r3==(-10)]=0 # 1 is vegetation and 0 is the rest

# transform the binary image into a raster where every vegetation patch has an ID
p=rasterToPolygons(r3, dissolve=T) # turns the raster into polygons, dissolve set as TRUE will merge adjacent polygons of equal value
p=disaggregate(p) # disaggregate will separate non touching polygons of the same class
pr=rasterize(p,r3) # rasterize will transform the polygons into a raster where all classes have their own ID 
ps=PatchStat(pr) # ps will contain landscape patch statistics for every patch.

# and so on... 

### end of landscape metrics extraction
```

- the section is named  
- all functions are annotated to say why they are used  
- some choices are justified directly  
- the input and output are identified, and the paths are made in a way that can use any file names, as long as it has the right format and it is put inside the project's data folder
- there are some warnings about things that could change based on the nature of the input data


# Storage and Safekeeping

Another very most important element of reproducibility is being able to keep the original work.  
  
There are many different ways to safely keep your data and scripts.  


It can be done **locally**:  

- using redundancy on multiple devices (backups, local or network (server, NAS, etc.))  

- can be scheduled or done manually

- Ideally requires different physical locations (some dedication)  

Or **remotely.. with Caution**:  

```{r, echo=FALSE, out.width="40%", fig.align='left'}
cloud_logos = list.files("images/logos/", pattern ="*1.png$", full.names = TRUE)
include_graphics(cloud_logos)
```

```{r, echo=FALSE, out.width="20%", fig.align='left'}
cloud_logos = list.files("images/logos/", pattern ="*2.png$", full.names = TRUE)
include_graphics(cloud_logos) # test
```

Some services have paywalls or hard limits that can cause issues. Also, it should never be completely trusted that your files are safe on a remote server.

In most cases, both local and remote backups are necessary.  
- Local is useful for heavier files (very large datasets, high resolution images).  
- Remote allows easier access from different users, or different platforms, but might be unreliable.  
  
Some parts of the *same project*, such as large datasets, can be local only, while some others can be hosted remotely, such as scripts.  
  
**So what should be saved where and how?**  
Depends on project, collaborators, etc. (plan ahead)

## Exemple of bad data handling

At one point or another, most of us realize that the way we organize our scripts and projects is BAD.  

For me, it happened when I started collaborating with others...  
Multiple scripts with multiple collaborators for the same project.  
  
All of us had different versions of every code and every database in multiple locations. Those screenshots are from my laptop only, AFTER cleaning up.

Location 1

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[1])
```

Location 2

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[2])
```

Location 3

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[3])
```

Location 4

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[4])
```

Location 5

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[5])
```

That is just **not good enough** for collaborative projects because of the very fast versionning of every files, by multiple users.

# Version Control Systems (VCS)

One of the strongest solutions to those problems is the use of **version control systems**, such as *Git* (or many other, such as SVN, GitHub, etc). When combined with a clean project approach and the use of tools such as RMarkdown, it allows for the same project to be held within a very simple directory such as the following one :

```{r, echo=FALSE, out.width="60%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[6])
```

Where all users can stores files and track *changes* occuring in them, while keeping each others environment free of useless clutter.

## Git

```{r, echo=FALSE, out.width="20%"}
git_logo = list.files("images/logos/", pattern ="*3.png$", full.names = TRUE)
include_graphics(git_logo)
```

*Git* has its own simple language and can be implementend in most operating systems.  

It is based on **repositories** containing the latest updates *commited* by any user.  

**Commits** act as snapshots, which can always be reverted to.  

Being easily *decentralised*, it is especially effective for collaborative work using github, notably. 

## Basic Git workflow

1. Initialise / Clone / Fork / Open a repository   
2. Work  
3. Commit changes locally  
5. Push changes to remote repository (GitHub)  

To learn about the basic commands : https://www.youtube.com/watch?v=SWYqp7iY_Tc  
very good ~30min tutorial (the equivalent will be done LIVE during the workshop)

## Other Git related things

*GUI vs commands* : There are many environments that allow the use of Git. The most basic one is gitbash, where all commands must be written by hand. Some programs, like Rstudio, have their own implementation of Git within their own UI. It is also possible to have external programs to help you with git, such as gitkraken, or even github desktop, which facilitates the use of github on your personnal pc.

*.gitignore* : a text file named .gitignore in your repo will tell Git which files should be ignored  

*storage capacity* : While a git repository is generally quite light, github won't accept very large commits. do think about other ways of storing large files.  

*branching and merging* : Branching and merging are ways for different versions of a project to co-exist. I your team wants to develop multiple functions in a single workflow, one branch could be developped for every function, and then they can be merged together into another branch. this allows for parallel work, or simply to keep different versions intact. While this may be intimidating at first, playing with the branching function is quite easy and you will familiarise with it very fast.

```{r, echo=FALSE, out.width="70%"}
git_images = list.files("images/git/", full.names = TRUE)
include_graphics(git_images[1])
```


## exercise : Git bash

1. install Git and open gitbash on a new folder
2. configure Git (git config --global user.name "name" & git config --global user.email "your@email.com")
3. init a repo (git init)
4. stage/track (touch xx.html, git add, git status)
5. commit (git commit -m "message")
6. history (git log)
7. return (git checkout <hash>)
8. branching (git branch, git checkout, git branch <name>, git checkout)
9. merging branches (git merge <name>)

## GitHub

GitHub is basically a cloud for Git.  

It allows users to publish their repositories privately or publicly, allowing other users to clone them and collaborate.
It interacts very efficiently with the Git language and has a very nice GUI application (GitHub Desktop)

Specs: 
- no user disk quotas  
- hard limit of 100Gb / repo  
- hard limit of 100Mb / file (!)  

This means:  
- Bigger files should be kept *out* of GitHub (In any case, your bigger, original files should be backed locally by every collaborator)  
- Ligther files (subsets) should be used while building the code and working with GitHub (which makes sense for collaborative purposes)  

## Git as a language vs Git as a tool

The most basic Git implementations can be very difficult at first because Git is foremost a *programming language*.  
  
It can be difficult to bring a new language in your workflow without making the processing feel heavier than necessary.
  
Many alternatives to coding exist when using Git : RStudio, Github Desktop, etc. They can make Git much easier.

## Other honorable mentions

- Dropbox  
  
- RStudioCloud  

