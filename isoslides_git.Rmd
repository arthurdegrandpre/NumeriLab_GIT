---
title: "NumeriLab - Reproducibility and Collaborative coding"
author: "Arthur de Grandpr√©"
date: "20 mai 2019"
output:
  ioslides_presentation:
    css: styles.css
    logo: images/logo_rive_logo_txt_FR_ENG-01.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE, echo=FALSE, warning=FALSE, message=FALSE}
rm(list=ls())
gc()
library(png)
library(grid)
library(knitr)
library(ggplot2)
```

## Context {.build}

Previous workshops have been:  

- Dedicated to enhancing coding and data science level

- Largely based on the R environment  

- Giving strong numeracy bases for everyone attending  
  
But...  
I feel like we still have yet to tackle some of basics of *coding hygiene* like:  

- How to ensure reproducibility of analysis  

- How to ensure safekeeping of your scripts  

- How to collaborate efficiently in your code oriented projects  

## Planning {.build}

The goal of this workshop is that by the end of the hour, attendees are able to figure out their own recipe for efficient collaborative coding.  
  
To do so, we will go through:  

- Basic annotation tips  

- How to structure scripts and projects efficiently  

- How to ensure safekeeping of scripts (version control, backups, etc.)

- How to share your scripts and their results  
  
## Important note

```{r warningsign, echo=FALSE, fig.height=1.5, fig.width=2.5, fig.align='center'}
img1 = readPNG("images/warning.png")
grid.raster(img1)
```
   
There is no single way of doing all that's gonna be shown today.  
  
The following material is based on my personnal experience and what has helped me when working with collaborators.  
  
This is mostly an effort to make sure everyone has some basic reflexes when working on their scripts.  

## Reproducibility

```{r bad annotation, eval=FALSE}

r1=brick("data/test_CIR_clip_01.tif", crs="+init:epsg=26918") ; r1=r1/255
r1$ndvi=(r1$test_CIR_clip_01.1-r1$test_CIR_clip_01.2)/(r1$test_CIR_clip_01.1+r1$test_CIR_clip_01.2)
r2[r2$ndvi<(-0.45) | r2$test_CIR_clip_01.3>0.42]=-10
r3=r2$ndvi
r3[r3>(-10)]=1 ; r3[r3==(-10)]=0
p=rasterToPolygons(r3, dissolve=T)
p=disaggregate(p)
pr=rasterize(p,r3)
ps=PatchStat(pr)
p@data$patchID=rownames(p@data)
p2=merge(p,ps, by="patchID") ; p2=p2[complete.cases(p2@data),]
rc=rasterize(p2, r3, field="ndvi")
pv=p2[p2@data$ndvi==1,]
pr2=rasterize(pv, r3)
PS = PatchStat(pr2)
CS = ClassStat(rc)

```


## Reproducibility (2)
There are many ways to tackle reproducibility of an analysis, the easiest is to think your code for it since $T_0$.  
  
This can be done by: 

- Working by **projects**
- Not procrastinating on the use of # everywhere in your code (don't hesitate to overdescribe, discuss results)  
- Planning your code like you would plan a written document (flowing, logical process)  
- Using R Markdown (Thanks *Lucas*)  
- Structuring your code in the most generic way possible (the less specific you are, the easier it will be to transfer your code)
- Writing your own functions, or even packages  

## Reproducibility (3)

**Bottom line** : Code for future you if not for others

## Reproducibility exemples

(external)

```{r exemple 1, eval=FALSE}

r1=brick("data/test_CIR_clip_01.tif", crs="+init:epsg=26918") ; r1=r1/255
r1$ndvi=(r1$test_CIR_clip_01.1-r1$test_CIR_clip_01.2)/(r1$test_CIR_clip_01.1+r1$test_CIR_clip_01.2)
r2[r2$ndvi<(-0.45) | r2$test_CIR_clip_01.3>0.42]=-10
r3=r2$ndvi
r3[r3>(-10)]=1 ; r3[r3==(-10)]=0
p=rasterToPolygons(r3, dissolve=T)
p=disaggregate(p)
pr=rasterize(p,r3)
ps=PatchStat(pr)
p@data$patchID=rownames(p@data)
p2=merge(p,ps, by="patchID") ; p2=p2[complete.cases(p2@data),]
rc=rasterize(p2, r3, field="ndvi")
pv=p2[p2@data$ndvi==1,]
pr2=rasterize(pv, r3)
PS = PatchStat(pr2)
CS = ClassStat(rc)

```


## Storage and Safekeeping

The most important element of replicability is being able to keep the original work.  
  
There are many different ways to backup your data and scripts.  


It can be done **locally**:  

- using redundancy on multiple devices (backups)  

- can be programmed weekly

- requires different hard drives in different physical locations (some dedication)  

## Storage and Safekeeping (2)

Or **remotely**:  

```{r, echo=FALSE, out.width="40%"}
cloud_logos = list.files("images/logos/", pattern ="*1.png$", full.names = TRUE)
include_graphics(cloud_logos)
```

```{r, echo=FALSE, out.width="25%"}
cloud_logos = list.files("images/logos/", pattern ="*2.png$", full.names = TRUE)
include_graphics(cloud_logos)
```


## Storage and Safekeeping (3)

In most cases, both local and remote backups are necessary.  
- Local is useful for heavier files (very large datasets, high resolution images).  
- Remote allows easier access from different users, or different platforms.  
  
Some parts of the *same project* can be local only, while some others can be hosted remotely.  
  
**So what should be saved where and how?**

## exemple 2

Most of us, at one point or another, had to realize that the way we organize our scripts and projects is sub-optimal.  

For me, that happened two years ago, when I had to make multiple scripts with multiples collaborators for the same project.  
  
We all had different versions of every code and every database in multiple locations.

## location 1

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[1])
```

## location 2

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[2])
```

## location 3

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[3])
```

## location 4

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[4])
```

## location 5

```{r, echo=FALSE, out.width="85%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[5])
```

And this is after cleaning up... and only on my own laptop.  
  
But that's just not good enough for collaborative projects because of the very fast versionning of every files.

## Version Control Systems (VCS)

One of the solutions to those problems, is the use of **version control systems**, such as *Git* (or many other, such as SVN, GitHub, etc).

```{r, echo=FALSE, out.width="60%"}
script_images = list.files("images/scripts/", pattern ="*.jpg$", full.names = TRUE)
include_graphics(script_images[6])
```

Stores any files and tacks *changes* occuring in them.

## Git

```{r, echo=FALSE, out.width="20%"}
git_logo = list.files("images/logos/", pattern ="*3.png$", full.names = TRUE)
include_graphics(git_logo)
```

*Git* has its own simple language and can be implementend in most operating systems.  

It is based on **repositories** containing the latest updates *commited* by any user.  

**Commits** act as snapshots, which can always be reverted to.  

Being *decentralised*, it is especially effective for collaborative work. 

## Git workflow

1. Clone / Pull / Fork/ Initialise a repository   
2. work  
3. commit changes locally  
5. push changes to remote repository (GitHub)  

To learn about the basic commands : https://www.youtube.com/watch?v=SWYqp7iY_Tc very good ~30min tutorial

## Other Git related things

*staging*
*.gitignore*
*storage capacity*
*branching and merging*

*GUI vs commands*

## Version Control : Branches

**2:** Allows users to work on different versions of the same repository called **branches**.  
Those can be merged back into the original (Master) branch (or not).

```{r, echo=FALSE, out.width="60%"}
git_images = list.files("images/git/", full.names = TRUE)
include_graphics(git_images[1])
```

## GitHub

GitHub is basically a cloud for Git.  

It allows users to publish their repositories privately or publicly, allowing other users to clone them and collaborate.
It interacts very efficiently with the Git language and has a very nice GUI application (GitHub Desktop)

Specs: 
- no user disk quotas
- hard limit of 100Gb / repo
- hard limit of 100Mb / file (!)

This means:
- Bigger files should be kept *out* of GitHub (In any case, your bigger, original files should be backed locally by every collaborator)
- Ligther files (subsets) should be used while building the code and working with GitHub (which makes sense for collaborative purposes)
- This is where .gitignore becomes useful

## Honorable mentions

- Dropbox

- RStudioCloud